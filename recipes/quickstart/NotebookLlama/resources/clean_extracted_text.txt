Tao Shen4, Reynold Cheng1, Jinyang Li1, Can Xu5, Dacheng Tao6, Tianyi Zhou2 1The University of Hong Kong2University of Maryland3Microsoft 4University of Technology Sydney5Peking University6The University of Sydney shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu ckcheng@cs.hku.hk
examined through a three-pillar framework, including algorithm, skill, and verticalization, providing a comprehensive examination of knowledge representation mechanisms, skill enhancement, and their practical implications across various fields.

Data augmentation (DA) plays a crucial role in knowledge representation, as it enables the creation of context-rich, skill-specific training data, thereby bolstering the performance of Large Language Models (LLMs). This paradigm emerges as a powerful tool within the knowledge domain, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of proprietary models.

The work aims to provide a detailed overview of current methodologies in knowledge representation, highlighting the application of data augmentation to improve model performance, as well as the potential benefits of skill-specific training data in achieving LLMs' proprietary characteristics.
ecent years. Proprietary Large Language Models such as GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI et al., 2023), Gemini (Team et al., 2023) and Claude2 have demonstrated impressive capabilities in various NLP tasks, including language understanding, text generation, and dialogue systems.
ophisticated problem-solving capabilities. The core significance of these LLMs lies in their emergent abilities, where models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency.
compared to open-source alternatives. These models' capabilities are often outweighed by their restricted accessibility and high costs, making them inaccessible to individuals and small organizations. The added burden of substantial usage fees and limited access to data privacy and security concerns arise, especially when handling sensitive information.
source models like LLaMA (Touvron et al., 2023) and Mistral (Jiang et al., 2023a). One of the primary benefits of open-source models is their accessibility and adaptability, as they are available to a broader range of users, from individual researchers to smaller organizations, without the constraints of licensing fees or restrictive usage policies. This fosters a more collaborative and inclusive AI research environment, encouraging innovation and diverse applications. Additionally, the customizable nature of open-source LLMs allows for more tailored solutions, addressing specific needs that generic, large-scale models may not meet.
results in lower performance on real-world tasks with a lot of instructions (Zheng et al., 2023a). These models, with fewer parameters, struggle to capture the depth and breadth of knowledge embodied in larger models like GPT-4.
can bridge the gap with knowledge distillation techniques. Knowledge distillation involves leveraging proprietary models' advanced capabilities to enhance open-source models' competencies. This process is similar to transferring knowledge from a skilled teacher to a student, where the student (open-source model) learns to mimic the teacher's performance characteristics.
mpt the LLM to generate more data with respect to a specific skill or domain.

KD retains its fundamental role in compressing LLMs, making them more efficient without significant loss in performance.

More recently, the strategy of employing open-source LLMs as teachers for their own self-improvement has emerged as a promising approach, enhancing their capabilities significantly.

Figure 1 provides an illustration of these three key roles played by KD in the context of LLMs.

KD plays three key roles in LLMs: Primarily enhancing capabilities, offering traditional knowledge, and self-improvement.
' ability to adapt to specific domains and tasks, and enhancing their overall performance in various applications, including but not limited to, self-improvement via knowledge acquisition and learning, a growing trend in the field of artificial intelligence.
ceted and transformative. This process narrows the gap between proprietary and open-source models, streamlining computational requirements and enhancing environmental sustainability of AI operations. Additionally, it fills the gap between open-source models and proprietary ones.
illation of LLMs arises from the rapidly evolving landscape of AI and the increasing complexity of these models. As AI continues to penetrate various sectors, the ability to efficiently and effectively distill knowledge from proprietary LLMs to open-source ones becomes a practical necessity. This demand is driven by the growing demand for accessible, cost-effective, and adaptable AI solutions that can cater to a diverse range of applications.
uration x,yrawsynthesisefeedbackfeedback input outputself-knowledge outputinputinput label labeling expansion x,ydemonstrationsexpandfeature featureinput,outputextract Sec.4Sec.5 Sec.3.1Sec.3.2 ①②③④ fig 2: An overview of this survey on knowledge distillation of large language models
ehensive sections, each designed to offer a deep dive into the multifaceted aspects of knowledge distillation within the realm of LLMs. Following this introduction, §2 provides a foundational overview of knowledge distillation, comparing traditional techniques with those emerging in the era of LLMs and highlighting the role of data augmentation (DA) in this context.

§3 delves into the approaches to elicit knowledge from teacher LLMs and core distillation algorithms, examining methods from supervised fine-tuning to more complex strategies involving divergence and similarity, reinforcement learning, and ranking optimization. Then, §4 focuses on skill distillation, exploring how student models can be enhanced to improve context understanding, alignment with user intentions, and performance across a variety of NLP tasks. This includes discussions on natural language understanding.
smaller, more manageable size, often through machine learning techniques.

2023), Orca 2 (Mitra et al., 2023), Baize (Xu et al., 2023b), Mammoth (Yue et al., 2023a), MixedDistill (Chenglin et al., 2023), Self-Instruction (Wang et al., 2022a), Alpaca (Taori et al., 2023), CodeAlpaca (Chaudhary, 2023), SelfAlign (Sun et al., 2024b), WizardLM (Xu et al., 2023a), WizardCoder (Luo et al., 2023a), WizardMath (Luo et al., 2023b), AugGPT (Dai et al., 2023a), TDG (He et al., 2023b), CurationUltraChat (Ding et al., 2023b), Phi-1 (Gunasekar et al., 2023), Phi-1.5 (Li et al., 2023a), Phi-2 (Mar, 2023), Magicoder (Wei et al., 2023), WaveCoder (Yu et al., 2024), ZeroGen (Ye et al., 2022), SunGen (Gao et al., 2023a), InPars (Bonifacio et al., 2022), FeatureBabyLlama (Timiryasov and Tastet, 2023), MiniLLM (Gu et al., 2024), GKD (Agarwal et al., 2024), QuantGPT (Tao et al., 2022a), LLM-QAT (Liu et al., 2023a), FeedbackCAI (Bai et al., 2022a), UltraFeedback (Cui et al., 2023a), Zephyr
edgeSelf-Instruct et al., 2022a, Self-Align et al., 2024b, RLCD et al., 2024, ImpDistill et al., 2023, LMSI et al., 2023a, ReST et al., 2023, Self-Rewarding et al., 2024a, Baize et al., 2023b, STaR et al., 2022, DistillationSupervised Fine-TuningAlpaca et al., 2023, Vicuna et al., 2023, WizardLM et al., 2023a, Self-Instruct et al., 2022a, Baize et al., 2023b, STaR et al., 2022, Divergence and SimilarityDistilGPT et al., 2019, f-Distill et al., 2023, MiniLLM et al., 2024, TED et al., 2023, GKD et al., 2024, BabyLlama et al., 2023, Reinforcement LearningCAI et al., 2022a, UltraFeedback et al., 2023, WizardMath et al., 2023
phyr (Tunstall et al., 2023), CycleAlign (Hong et al., 2023), Skill DistillationContext FollowingInstruction FollowingSelf-Instruct (Wang et al., 2022a), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), WizardLM (Xu et al., 2023a), Orca (Mukherjee et al., 2023), Orca 2 (Mitra et al., 2023), WizardMath (Luo et al., 2023b), Llama-GPT4 (Peng et al., 2023a), Multi-turn DialogueVicuna (Chiang et al., 2023), Baize (Xu et al., 2023b), UltraLLaMA (Ding et al., 2023b), CAMEL (Li et al., 2023b), OpenChat (Wang et al., 2023c), Zephyr (Tunstall et al., 2023), RAG Capbility KARD (Kang et al., 2023a), Self-RAG (Asai et al., 2023), AlignmentThinking PatternSelfee (Ye et al., 2023), Orca (Mukherjee et al., 2023), Orca 2 (Mitra et al., 2023), AFT (Wang et al., 2023d), AdaptLLM (Cheng et al., 2023), KnowPAT (Zhang et al., 2023a), PreferenceCAI (Bai et al., 2022a)
23), RLAIF (Lee et al., 2023a), Zephy (Tunstall et al., 2023), UltraFeedback (Cui et al., 2023a), ValueCAI (Bai et al., 2022a), Align Honesty (Yang et al., 2023a), SANDBOX (Liu et al., 2023b), Self-Align (Sun et al., 2024b), UltraFeedback (Cui et al., 2023a), RLCD (Yang et al., 2024), AgentTool UsingToolformer (Schick et al., 2023), Graph-ToolFormer (Zhang, 2023), Gorilla (Patil et al., 2023), ToolAlpaca (Tang et al., 2023a), ToolLLM (Qin et al., 2023a), CRAFT (Yuan et al., 2023a), Confucius (Gao et al., 2023b), MLLM-Tool (Wang et al., 2024), α-UMi (Shen et al., 2024), PlanningFireAct (Chen et al., 2023b), AgentTuning (Zeng et al., 2023a), Lumos (Yin et al., 2023a), AUTOACT (Qiao et al., 2024), TPTU-v2 (Kong et al., 2023), NLP Task SpecializationNLUAugGPT (Dai et al., 2023a), GPT Annotation (Gilardi et al., 2023), (Ding et al., 2023a), TDG (He et al., 2023b), SunGen (Gao et al., 2023a)
(Ye et al., 2022), NLGInheritSumm (Xu et al., 2023c), RECOMP (Xu et al., 2024b), MaRio (Ramnath et al., 2023), ID (Jung et al., 2023), GPT-3 Labeling (Wang et al., 2021b), BioGPT (Guo et al., 2023a), ChatGPT NMT (Yang and Nicolai, 2023), Information RetrievalQUILL (Srinivasan et al., 2022), Promptgator (Dai et al., 2023b), InPars (Bonifacio et al., 2022), AugTriever (Meng et al., 2023), RankVicuna (Pradeep et al., 2023a), RankZephyr (Pradeep et al., 2023b), ExaRanker (Ferraretto et al., 2023), Recommendation NDR (Mysore et al., 2023), InstructRec (Zhang et al., 2023b), ONCE (Liu et al., 2023c), Text Generation EvaluationPandaLM (Wang et al., 2023b), Prometheus (Kim et al., 2024), InstructScore (Xu et al., 2023d), TigerScore (Jiang et al., 2023c), Auto-J (Li et al., 2024a), CodeCodeAlpaca (Chaudhary, 2023), CodeLlama (Rozi `ere et al., 2023), Magicoder (Wei et al., 2023)
r (Yu et al., 2024), Code Clean (Jain et al., 2023), Multi-ModalityLLaVA (Liu et al., 2023e), SVIT (Zhao et al., 2023b), LVIS-Instruct4V (Wang et al., 2023e), Shikra (Chen et al., 2023c), LSKD (Park et al., 2023), DetGPT (Pi et al., 2023; Zhao et al., 2023c), LRV (Liu et al., 2023f), NExT-GPT (Wu et al., 2023b), Valley (Luo et al., 2023d), ILuvUI (Jiang et al., 2023d), StableLLaVA (Li et al., 2023c), PointLLM (Xu et al., 2023e), Verticalization DistillationLaw (Huang et al., 2023b; Cui et al., 2023b); Medical & Healthcare (Zhang et al., 2023c; Chen et al., 2023d); Finance (Zhang and Yang, 2023); Science (Xie et al., 2023a; Zhang et al., 2024)
g, where the student learns from the softened softmax output of the teacher, the focus has shifted from mere architecture compression to knowledge elicitation and transfer in the knowledge distillation landscape.
uage models like LLMs, allowing for a more targeted and flexible approach to knowledge elicitation. This is achieved through carefully designed prompts that tap into the model's capabilities, enabling a more efficient and effective extraction of knowledge.
alignment, and value alignment, whereas earlier techniques concentrated on output replication, indicating a shift towards a more holistic and comprehensive transfer of cognitive capabilities. This shift is accompanied by the emulation of thought processes and decision-making patterns of the teacher model, involving complex strategies like chain-of-thought prompting, to enhance problem-solving and decision-making capabilities.

tegies, DA facilitates a more effective transfer of knowledge, focusing on qualitative aspects of learning, rather than quantitative expansion. This strategic use of DA within KD processes empowers open-source models to approximate contextual adeptness, ethical alignment, and deep semantic insights, democratizing access to advanced AI capabilities, and fostering innovation across a broader spectrum of applications and users.
ndscape of knowledge distillation within the context of LLMs, following a meticulously structured taxonomy as in Figure 3. The survey's scope is delineated through three primary facets: Knowledge Distillation, Skill Distillation, and Verticalization Distillation. Each facet encapsulates a range of subtopics and methodologies.

Knowledge Distillation is a critical component of the overall process, providing the technical foundations for both Skill Distillation and Verticalization Distillation. This segment focuses on the technical foundations and methodologies of knowledge distillation, including the processes involved in constructing knowledge from teacher models and integrating this knowledge into student models. Strategies such as labeling and expansion are also explored.
chanisms (Tunstall et al., 2023), and self-knowledge generation (Wang et al., 2022a). this exploration seeks to uncover the various ways knowledge can be identified, expanded, and curated for effective distribution. the 'distillation' subsection examines learning approaches like supervised fine-tuning (SFT) (Wang et al., 2022a), divergence minimization (Agarwal et al., 2024), and reinforcement learning techniques (Cui et al., 2023a), demonstrating how KD enables open-source models to obtain knowledge from proprietary ones.
et al., 2023, the survey investigates thinking patterns, persona/preference modeling, and value alignment The agent category delves into skills such as tool using and planning NLP task specialization Dai et al., 2023a; Jung et al., 2023; Chaudhary, 2023, is scrutinized through lenses like natural language understanding NLU, natural language generation information retrieval recommendation systems text generation evaluation and code generation Finally, the survey addresses multi-modality Liu et al., 2023e; Zhao et al., 2023b, exploring how KD enhances LLMs ability to integrate multiple forms of input Verticalization Distillation This section assesses the application of KD across diverse vertical domains offering insights into how distilled LLMs can be tailored for specialized fields such as Law LAW 2023, Medical & Healthcare Wang et al.,
on not only showcases the practical implications of KD techniques but also highlights their transformative impact on domain-specific AI solutions. Through these facets, this survey provides a comprehensive analysis of KD in LLMs, guiding researchers and practitioners through methodologies, challenges, and opportunities in this rapidly evolving domain.

Declaration. This survey represents our earnest effort to provide a comprehensive and insightful overview of knowledge distillation techniques applied to LLMs, focusing on algorithms, skill enhancement, and domain-specific applications. Given the vast and rapidly evolving nature of this field, especially with the prevalent practice of eliciting knowledge from training data across academia, we acknowledge that this manuscript may not encompass every pertinent study or development. Nonetheless, it endeavors to introduce the state of knowledge in this field.
across a range of applications. Distillation Pipeline in LLM Era SeedKnowledgeSkill/Domain TeacherLLMKnowledgeElicitationStudentModelDistillationAlgorithmsteer driveGeneratedKnowledgeLearningObjectivetrain
directing the teacher LLM towards a specific target skill or domain. This is achieved through carefully crafted instructions or templates that guide the LLM's focus. These instructions elicit responses that demonstrate the LLM's proficiency in a particular area, such as a specialized domain like healthcare or law, or a skill like reasoning or language understanding. 

Seed Knowledge as Input. Once the target area is defined, the next step is to feed the teacher LLM with seed knowledge. This seed knowledge typically comprises a small dataset or specific data clues relevant to the target skill or domain knowledge from the teacher LLM. This serves as a catalyst, prompting the teacher LLM to generate more elaborate and detailed outputs based on this initial information.
generates knowledge examples. These examples are predominantly in the form of question-and-answer (QA) dialogues or narrative explanations, aligning with the natural language processing/understanding capabilities of the LLM. In certain specialized cases, the outputs may also include logits or hidden features, although this is less common due to the complexity and specific requirements of such data forms. The generated knowledge examples constitute the core of the distilled knowledge, encapsulating the advanced understanding and skills of the teacher's LLM. The process involves training the student model with a specific learning objective, which is to leverage the generated knowledge examples to train the model.
ating or adapting the knowledge from the teacher model. By minimizing this loss, the student model learns to emulate the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. The process involves iteratively adjusting the student model's parameters to reduce the gap between its outputs and those of the teacher model, ensuring effective transfer of knowledge.

knowledge.
ervoirs for distillation efforts. Numerous works have sought to harness the capabilities of powerful LLMs as teachers for annotating dataset samples across a range of tasks. For instance, efforts in natural language understanding involve using LLMs to categorize text. In natural language generation, LLMs assist in generating sequences for outputs. Text generation evaluation tasks leverage LLMs to label results. Reasoning tasks utilize LLMs for labeling Chains of Thought (CoT) explanations.
udent models to solve tasks in a more flexible way by following instructions. Collections of various NLP tasks, complemented by instructional templates, serve as valuable input sources for training. 

**FLAN-v2 Collections**

FLAN-v2 collections provide extensive publicly available sets of tasks with instructions, labeled with responses generated by teacher LLMs in Orca (Mukherjee et al., 2023; Mitra et al., 2023). These collections lack diversity and may have gaps between human’s natural queries.

**Real Conversations with Chat Models**

Real conversations between humans and chat models provide large-scale data with real queries and generations labeled by powerful LLMs, like ShareGPT. This data is useful for training models to handle real-world scenarios.

**Labeling Process**

Labeling could be guided by the quality of the data, which is often lacking in diversity and accuracy. This can lead to suboptimal model performance.
t (CoT) Prompt

A commonly used in-struct-ution for guiding labeling is the chain-of-thought (CoT) prompt. This involves guiding the model to generate a detailed and long answer by breaking down the problem or task into smaller subtasks and explaining each step in detail.
et al., 2023; Ji et al., 2023a; Luo et al., 2023b; Wu et al., 2023c; Sun et al., 2024b; Xu et al., 2023a; Guo et al., 2023c; Rozi et al., 2023; West et al., 2022). These methods take the demonstrations as seed knowledge and aim to expand a large-scale and diverse data by in-context learning. A key characteristic of these expansion methods is the utilization of the in-context learning ability of large language models (LLMs) to generate data similar to the provided demonstrations.
ation of different knowledge elicitation methods from teacher LLMs" 𝑦! 𝑦# 𝑥 𝑥& 𝑦" 𝑥𝑦𝐼𝑦 𝑦𝑥𝑦𝐴𝑥𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦𝑦
uction: ")
while True:
    if x == "iterative bootstrapping":
        # This is the new input
        # Add it to the instruction pool
        new_pool = input("Enter the new pool: ")
        # Add the new pool to the initial pool
        initial_pool = new_pool
        break
    elif x == "Self-Instruct":
        # This is the new instruction
        # Add it to the instruction pool
        new_pool = input("Enter the new pool: ")
        # Add the new pool to the initial pool
        initial_pool = new_pool
    elif x == "Taori et al. (2023)":
        # This is the new instruction
        # Add it to the instruction pool
        new_pool = input("Enter the new pool: ")
        # Add the new pool to the initial pool
        initial_pool = new_pool
    else:
        # This is the new input
        # Add it to the instruction pool
        new_pool = input("Enter the new pool: ")
        # Add the new pool to the initial pool
        initial_pool = new_pool
    # Generate the output
    y = input("Enter the output: ")
    # Print the new input and output
    print("new_input =", new_input)
    print("output =", y)
    # Check if the output is "iterative bootstrapping"
    if y == "iterative bootstrapping":
        # Print the new input
        print("new_input =", x)
        # Print the output
        print("output =", y)
        # Add the new input to the instruction pool
        new_pool = input("Enter the new pool: ")
        # Add the new pool to the initial pool
        initial_pool = new_pool
    # Check if the output is "Self-Instruct"
    elif y == "Self-Instruct":
        # Print the new input
        print("new_input =", x)
        # Print the output
        print("output =", y)
        # Add the new input to the instruction pool
        new_pool = input("Enter the new pool: ")
        # Add the new pool to the initial pool
        initial_pool = new_pool
    # Check if the output is "Taori et al. (2023)"
    elif y
her LLM to generate instructions corresponding to some specific topics. Xu et al. (2023a) propose an Evol-Instruct method to expand the instructions from two dimensions: difficulty and diversity. This method is domain-agnostic and has been used to expand the distillation of coding (Luo et al., 2023a) and math (Luo et al., 2023b). This approach can significantly augment NLP task datasets with similar samples, thereby enhancing task performance. AugGPT (Dai et al., 2023a) leverages a teacher LLM to rephrase each sentence in the training samples into multiple conceptually similar, but semantically varied, samples. Similarly, TDG (Heet al., 2023b) proposes the Targeted Data Generation (TDG) framework, which
ta and generates new samples for these subgroups using LLMs through in-context learning. In summary, the expansion method leverages the in-context learning strengths of LLMs to produce more varied and extensive datasets with both inputs and outputs. However, the quality and diversity of the generated data are heavily reliant on the teacher LLMs and the initial seed demonstrations. This dependence can lead to a dataset with inherent bias from LLMs and a homogeneity issue where the generations may be prone to similarity, ultimately limiting the diversity this method seeks to achieve. Moreover, the expansion process may inadvertently amplify any biases present in the seed data.
by curating high-quality data through extensive meta-information. This approach involves sourcing seed knowledge from task datasets, which can introduce noise and variability in data quality. In contrast, expansion methods generate data from seed demonstrations, resulting in homogeneous data when scaled to large quantities. To address these challenges, data curation involves curating high-quality or large-scale data by incorporating diverse metadata. This process enables the generation of controllable features, such as topics or knowledge points, from scratch.
5) in this formulation, mrepresents the diverse meta-information used to guide the synthesis of x, and Iis the instruction guiding teacher LLMs to generate xory. different studies primarily vary in their source and method of leveraging meta-information. ultraChat (Ding et al., 2023b) effectively demonstrates the process of curating both high-quality and diverse data by distilled knowledge. they collect extensive meta-information across three domains: questions about the world, creation and generation, and assistance on existing materials.


ch is a key aspect of modern AI.
Large Language Models (LLMs) trained on annotated output sequences. These annotations are then distilled into the student model using methods such as Kullback-Leibler Divergence (KLD). The process of eliciting feature knowledge can be formulated as: (x, y, ϕfeat(x, y;θT))|(x∼ X, y∼ Y). (7) In this formulation, Y is the output set, which can be generated by teacher LLMs, the student model, or sourced from the dataset. ϕfeat(·;θT) represents the operation of extracting feature knowledge from the teacher LLM. The most straightforward method to elicit feature knowledge of teacher LLM is to label a fixed dataset of sequences with token-level probabilities
nowledge in intermediate layers of the teacher model, TED (Liang et al., 2023a) designs task-aware layer-wise distillation. They align the student’s hidden representations with those of the teacher at each layer, selectively extracting knowledge pertinent to the target task.

Gu et al. (2024) and Agarwal et al. (2024) introduce a novel approach where the student model first generates sequences, termed ‘self-generated sequences.’ The student then learns by using feedback (i.e. output distribution) from teacher on these sequences.

This method is particularly beneficial when the student model lacks the capacity to mimic teacher’s distri-bution. Moreover, various LLM-quantization methods with distilling feature knowledge from teacher LLMs have been proposed (Tao et al., 2022a; Liu et al., 2023a; Kim et al., 2023b). These methods aim to preserve the original output distribution when
nt source for multi-teacher knowledge distillation. Timiryasov and Tastet (2023) leverages an ensemble of GPT-2 and LLaMA as teacher models to extract output distributions. Similarly, FuseLLM (Wan et al., 2024a) inno- vatively combines the capabilities of various LLMs through a weighted fusion of their output distributions, integrating them into a singular LLM. This approach has the potential 10 to significantly enhance the student model’s capabilities, surpassing those of any individual teacher LLM.
relationship, whereas internal parameters are inaccessible. Furthermore, student models from white-box LLMs may struggle to match the complexity and nuance of black-box teacher LLMs. 3.1.5 Feedback is a crucial aspect of training, but most previous works focus solely on teacher-student feedback, neglecting the teacher's perspective. The teacher's feedback typically involves providing preferences, assessments, or corrective information, such as ranking student outputs and distilling this preference into the model through Reinforcement Learning from AI Feedback (RLAIF)
viding feedback from teacher LLMs
This operation evaluates the student's output y given the input x, by offering assessment, corrective information, or other forms of guidance
This feedback knowledge cannot only be distilled into the student to also generate feedback such as creating a student preference model
More importantly, it enables the student to refine its responses based on the feedback
Various methods have been explored to elicit this advanced knowledge
Bai et al. 2022a
Luo et al. 2023b
Cui et al. 2023a
Kwon et al. 2023
Jiang et al. 2023b
Chen et al. 2023a
Gu et al. 2024
Agarwal et al. 2024
Chen et al. 2024b
Guo et al. 2024
Ye et al. 2023
Hong et al. 2023
Lee et al. 2023a
harmlessness preferences from LLMs by training an SFT-trained LLM to generate response pairs for each prompt, then ranking them for harmlessness to create a preference dataset. This dataset is distilled into a Preference Model (PM), which guides the RL training of a more harmless LLM policy. 

Wizard- Math (Luo et al., 2023b) places emphasis on mathematical reasoning by employing ChatGPT as a teacher to directly provide process supervision and evaluate the correctness of each step in the generated solutions. To scale up high-quality distilled preference data, Cui et al. (2023a) develop a large-scale preference dataset for distilling better models, UltraFeedback. They compile various instructions and models to produce comparative data. 

GPT-4 is used to score candidates from various aspects of preference, including instruction-following and truthfulness.
ide extensive feedback on instances where students struggle. Instructions can be pinpointed that pose challenges to the student model, generating new, more difficult ones aimed at improving abilities. Refinement feedback on incorrect code snippets is offered by PERsD. Similarly, SelFee utilizes ChatGPT to revise student's answers based on specific execution errors. In contrast, FIGA compares student's response to ground-truth response. Furthermore, teacher model's distribution over student's generations can serve as a form of feedback. MiniLLM and GKD present innovative strategies.
on as feedback, this feedback loop enables the teacher to refine the student model's learning process, effectively eliminating the need for a proprietary, powerful external teacher model. The teacher model acts as both teacher and student, iteratively improving itself by distilling and refining its own generated outputs, bypassing traditional methods' limitations. This self-elicited knowledge is formulated as a generalized function, (D(sk)={(x, y, ϕsk(x, y))|x∼ S, y∼pS(y|I⊕x)}, where ϕsk(·) represents an additional process to the student's outputs.
e, demonstrating its potential for creating more efficient and autonomous learning systems, including Self- Instruct, which utilizes GPT-3 for data augmentation through the Expansion approach, generating additional data samples to enhance the dataset, fine-tuning the original model to improve its performance
by modifying prompts to generate in-depth and detailed responses. Context distillation (Askell et al., 2021) distills these responses with non-verbose instructions back to the model, refining their understanding.
nsistent answers. This process is repeated iteratively, improving the student model's knowledge over time. The Reinforced Self-Training (ReST) framework alternates between growth and improvement stages, where the student model generates multiple predictions, is ranked and filtered, and fine-tunes the language model on a curated dataset.
he language model is fine-tuned to differentiate self-generated responses from human-annotated data. Self-Rewarding (Yuan et al., 2024a) explores a novel approach utilizing the language model itself as a reward model. The model is prompted to autonomously assign rewards for self-generated responses, which can then be iterated to improve instruction following and reward modeling capabilities. 

Distillation is a key aspect of this process. This section delves into various methodologies for transferring knowledge from teacher LLMs to student models. Methods include strategies that enhance imitation by fine-tuning, divergence, and similarity, as well as advanced techniques like reinforcement learning.
r called Sequence-Level KD (SeqKD) (Kim and Rush, 2016), is the simplest and one of the most effective methods for distilling powerful black-box Divergence Type D(p, q)Function Forward KLDPp(t) logp(t) q(t) Reverse KLDPq(t) logq(t) p(t) JS Divergence1 2Pp(t) log2p(t) p(t)+q(t)+Pq(t) log2q(t) p(t)+q(t)**
formulated as minimizing the objective function: LSFT=Ex ~ X, y ~ pT(y|x)[−logpS(y|x)], 

where y is the output sequence produced by the teacher model. This simple yet highly effective technique forms the basis of numerous studies in the field. Numerous researchers have successfully employed SFT to train student models using sequences generated by teacher LLMs. 

SFT has been explored in many self-distillation works. Some representative ones include: 

Taori et al. 2023; Chiang et al. 2023; Wu et al. 2023c; Xu et al. 2023a; Luo et al. 2023b.
, represented by a general divergence function D: 
LDiv = ∫∞-∞ pT(y|x) d(pS(y|x)) 
Divergence-based methods minimize divergence between probability distributions of teacher and student models, represented by a general divergence function D: 
LDiv = ∫∞-∞ pT(y|x) d(pS(y|x)) 
those aimed at enhancing the similarity of hidden states 
Divergence-based methods aim to minimize divergence between probability distributions of teacher and student models, represented by a general divergence function D: 
LDiv = ∫∞-∞ pT(y|x) d(pS(y|x)) 
divergence-based methods 
divergence-based methods 
those minimizing divergence between probability distributions of teacher and student models, represented by a general divergence function D: 
LDiv = ∫∞-∞ pT(y|x) d(pS(y|x)) 
those aimed at enhancing the similarity of hidden states 
divergence-based methods 
Divergence-based methods aim to minimize divergence between probability distributions of teacher and student models, represented by a general divergence function D: 
LDiv = ∫∞-∞ pT(y|x) d(pS(y|x)) 
those minimizing divergence between probability distributions of teacher and student models, represented by a general divergence function D: 
LDiv = ∫∞-∞ pT(y|x) d(pS(y|x)) 
those aimed at enhancing the similarity of hidden states 
divergence-based methods 
Divergence-based methods 
those minimizing divergence between probability distributions of teacher and student models, represented by a general divergence function D: 
LDiv = ∫∞-∞ pT(y|x) d(pS(y|x)) 
those aimed at enhancing the similarity of hidden states 
divergence-based methods 
those minimizing divergence between probability distributions of teacher and student models, represented by a general divergence function D: 
LDiv = ∫∞-∞ pT(y|x) d(pS(y|x)) 
those aimed at enhancing the similarity of hidden states
2023; Timiryasov and Tastet, 2023; Liang et al., 2023a; Chen et al., 2024d) which forces aSto cover all the modes of a highly complex teacher, however, when a student model is unable to learn all modes of a highly complex teacher, the resultant mode-covering behavior might cause the student to assign probability mass to tokens with low probability under the teacher’s distribution (cf. Figure 6 blue curve). This mode-covering phenomenon can potentially lead to hallucinations and low-quality generations. Alternatively, mode-seeking divergences like reverse KL prioritize tokens where the teacher assigns high probabilities (cf. Figure 6 green curve). This approach can mitigate the risk of low-quality outputs, fostering more accurate generations. Gu et al. (2024) adopt reverse KL divergence to prevent students from overestimating
ization. Both Agarwal et al. (2024) and Sason and Verd' u (2016) assess the effect of different divergence functions in LLM distillation, finding the optimal divergence to be task-dependent. For instance, forward KL divergence is more suitable for tasks like Machine Translation, where the output has fewer modes or variations, while reverse KL divergence is preferable for tasks like dialogue generation and instruction tuning, which involve multiple modes and a wider range of potential responses.

The nature of the task significantly influences the selection of the divergence function for optimal performance. Similarity-based methods in knowledge distillation aim to align the hidden states or features of the student model with those of the teacher. These methods use various similarity metrics to measure and optimize the congruence of internal representations between the student and teacher.
ut. The teacher model is trained to optimize a specific objective function, which is not explicitly stated in the given text. However, based on the context, it is likely that the objective function is to minimize the difference between the teacher model's output and the student model's output, such that the student model produces outputs that are closer to the teacher model's outputs.

The student model uses task-aware filters to selectively capture the most pertinent information from the teacher model. These filters are designed to minimize the difference between the teacher model's output and the student model's output.

The similarity function used to compare the student model's output to the teacher model's output is likely to be a distance metric, such as cosine similarity or Euclidean distance. The distance metric measures the similarity between two vectors by calculating the cosine of the angle between them.

The following is the cleaned and processed text:

Two models, the teacher model and the student model, are used to generate the student model's output. The teacher model is trained to optimize a specific objective function, which is likely to be a distance metric, such as cosine similarity or Euclidean distance, to minimize the difference between the teacher model's output and the student model's output. The student model uses task-aware filters to selectively capture the most pertinent information from the teacher model, resulting in a more accurate output.
ity-based approaches are common in encoder-based LMs, their application in LLM knowledge distillation is not as widespread. However, considering their effectiveness, we anticipate an increase in research exploring these methods for LLM distillation in the near future. 3.2.3 Reinforcement Learning This section explores advanced methods of distilling knowledge into student models using reinforcement learning. This approach is especially relevant for leveraging feedback from teacher to train student models. The RL-based distillation process typically involves two main stages: Distilled Reward Model Training. The first stage
teacher LLMs. The typical feedback consists of input-output pairs (x, yw, yl), where ywandyl represent "winning" and "losing" outputs relative to the teacher's preferences. The loss function for the reward model is defined as: LRM(rϕ,D(fd)) = - E(x,yw,yl)∼D(fd)[logσ(rϕ(x, yw)−rϕ(x, yl))] (12) This formulation guides the reward model to distinguish between more and less preferable outputs based on the teacher's criteria. Instead of learning instance-level rewards, RLMEC (Chen et al., 2024b) adopts a different approach by training a generative reward model. This model is trained on erroneous solution rewriting data distilled from a teacher LLM.
the trained reward model. The student model is represented by a policy πθ, which is optimized to maximize the expected reward given the teacher model πref. The reward objective is given by: max θ E x∼X,y∼πθ(y|x)[rϕ(x, y)]−β DKL[πθ(y|x)∥πref(y|x)]
ard model. 3.2.4 Ranking Optimization Ranking optimization presents a stable and computationally efficient alternative to RL for injecting preference feedback into language models. This method, diverging from traditional RL approaches, directly incorporates ranking information into language models from a fixed preference dataset during fine-tuning. Intuitively, it directly updates policy to increase the relative likelihood of preferred responses. This direct optimization of preferences, without the need for sampling outputs, makes the process more stable and efficient.
expectation: E (x,yw,yl)∼D(fd) logπ(yw|x) πref(yw|x)−βlogπθ(yl|x) πref(yl|x), where ywis preferred over yl according to the teacher LLM.
comparison and ranking of responses based on the teacher's preferences. PRO (Song et al., 2023a) expands the concept of pairwise comparison to handle preference rankings of any length. For a given instruction x and a sequence of responses ordered by teacher preference as y1≻y2≻...≻yn, the RPO training objective is: LPRO=−n−1X k=1log(πk)Pn i=kexp(πi), where pk represents the conditional log probabilities for yi under the student policy πθ. By iteratively contrasting the likelihood of generating responses, PRO optimizes the student's LM to prioritize the most preferred response while progressively ranking the rest in the order of diminishing preference.
g, Alignment, Agent, NLP Task Specialization and Multi-Modality. Context Following focuses on the student's ability to comprehend and respond effectively to input information. Alignment delves into the student's capability to align its output with the teacher's responses. Moving forward, Agent underscores the autonomous nature of language models. NLP Task Specialization highlights the LLM's versatility in specializing across various Natural Language Processing tasks, demonstrating its adaptability. Finally, Multi-Modality encompasses the knowledge transfer from teacher LLMs to multi-modal models.
such as few-shot demonstrations, intricate instructions, and dialogue history — into smaller models has become a significant area of research effort. Many attempts are focused on imbuing smaller models with sophisticated, context-following capabilities. This discussion will delve into this aspect of skill distillation, categorizing it based on different types of context and exploring how each is distilled and incorporated into smaller, efficient models. 4.1.1 Instruction Following Instruction Following capacity enables LLMs to understand and follow user-given instructions, significantly enhancing human-AI interaction, allowing for seamless understanding and execution of tasks as directed by users. A primary method for acquiring this skill involves constructing instruction-like prompt-response pairs and employing Supervised Fine Tuning (SFT) for model training.
n-curated tasks
2. Taori et al. (2023) - 175 human-curated tasks
3. Wu et al. (2023c) - 175 human-curated tasks
4. Xu et al. (2023a) - 175 human-curated tasks
5. Alpaca Data - ChatGPT, LLaMA, Lion, Cata, BabyLlama, Dolly, MiniLLM
6. Timiryasov and Tastet (2023) - BabyLlama - Labeling, Expansion, Feedback
7. Gu et al. (2024) - GPT2, OPT, LLaMA, D&S MiniLLM
8. Sun et al. (2024b) - Self-Align - Feature D&S

Note: The text has been cleaned up to remove unnecessary characters, including new lines, LaTeX math, and extraneous text.
TaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT Llama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT Reflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT Selective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT Vicuna (Chiang et al., 2023) IF/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT Koala (Geng et al., 2023) IF/MD Human Conversation ChatGPT LLaMA Labeling SFT Baize (Xu et al., 2023b) IF/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT UltraChat (Ding et al., 2023b) IF/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT Orca (Mukherjee et al., 2023) IF/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT Orca2 (Mitra et al., 2023) IF/TP FLAN-v2 + Few-Shot/Math/Synthetic GPT4 LLaMA Labeling SFT SelFee (Ye)
et al., 2023) IF/TP e-SNLI + ANLI + CQA + SVAMP PaLM T5 Labeling SFT KnowPAT (Zhang et al., 2023a) IF/TP CPKG + QA Data ChatGPT + ChatGLM + Vicuna-7B LLaMA Labeling SFT DEBATunE (Li et al., 2024e) IF/TP Controversial Topics ChatGPT LLaMA Labeling SFT Phi-1 (Gunasekar et al., 2023) IF/Code - GPT3.5 phi-1 Curation SFT Phi-1.5 (Li et al., 2023a) IF/Code 20k Topics from Web GPT3.5 phi-1 Curation + Labeling SFT SAIL (Luo et al., 2023c) IF/RAG Alpaca Data + Web Content GPT4 LLaMA Label SFT KARD (Kang et al., 2023b) IF/RAG MedQAUSMLE ChatGPT T5 + OPT Label SFT + D&S Self-RAG (Asai et al., 2023) IF/RAG Open-Instruct GPT4 LLaMA Labeling SFT Alignment OpenChat (Wang et al., 2023c) IF/Preference Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT + RL Zephyr (Tunstall et al., 2023) IF/Preference Mixed Datasets GPT4 Mistral Labeling + Feedback SFT + RO ALMoST (Kim et al., 2023a)
ence Human-written Prompts LLaMA LLaMA Labeling SFT + RL RLAIF (Lee et al., 2023a) IF/Preference Human-written Prompts PaLM 2 PaLM 2 Labeling + Feedback RL GPT3 Reward (Kwon et al., 2023) Preference Human-written Prompts GPT3 GPT3 Labeling RL ILF (Scheurer et al., 2023) Preference Task-specific Datasets GPT3 + FeedME GPT3 Labeling RL ULTRAFEEDBACK (Cui et al., 2023a) Preference Mixed Datasets GPT4 LLaMA Labeling RL Constitutional AI (Bai et al., 2022a) Preference/Value Human-written Prompts Self-defined Student Model Self-defined Model Labeling + Expansion + Feedback SFT + RL SANDBOX (Liu et al., 2023b) Value Simulationtext-davinci-002/-003 + GPT4 + ChatGPTLLaMA Data Curation SFT + RL Agent Toolformer (Schick et al., 2023) Tool CCNet GPT-J GPT-J Labeling SFT Graph-ToolFormer (Zhang, 2023) Tool Mixed Graph Dataset ChatGPT GPT-J + LLaMA Labeling SFT Gorilla (Patil et al., 2023) Tool Online
PT LLaMA Curation + Expansion SFT ToolAlpaca (Tang et al., 2023a) Tool Public-apis Repository ChatGPT LLaMA Curation SFT ToolLLM (Qin et al., 2023a) Tool Real-world APIs ChatGPT LLaMA Curation SFT MLLM-Tool (Wang et al., 2024) Tool HuggingFace Model Cards GPT4 LLaMA Curation SFT FireAct (Chen et al., 2023b) Planning Mixed QA Dataset GPT4 LLaMA Labeling SFT AgentTuning (Zeng et al., 2023a) Planning 6 Agent Tasks GPT4 + ChatGPT LLaMA Labeling + Expansion SFT Lumos (Yin et al., 2023a) Planning Mixed Interactive Tasks GPT4 LLaMA Labeling SFT AUTOACT (Qiao et al., 2024) Planning Mixed QA Tasks LLaMA LLaMA Labeling SFT NLP Task Specialization AugGPT (Dai et al., 2023a) NLU Amazon/Symptoms/PubMed20k Dataset ChatGPT BERT Label SFT TDG (He et al., 2023b) NLU SST + QQP + MNLI GPT3 BERT Expansion SFT SunGen (Gao et al., 2023a) NLU Text Classification Tasks GPT2 DistilBERT Curation SFT
e++ Label SFT DIMSUM+ (Jung et al., 2023) NLG None GPT2 + CTRL + BioGPT T5 Curation + Self-Knowledge SFT Genie (Yehudai et al., 2024) NLG ELI5 + ASQA + NQ + CNN/DM Falcon + LLaMA FLAN + LLaMA Label SFT GKD (Agarwal et al., 2024) NLG/NLU/IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL QUILL (Srinivasan et al., 2022) IR IR Datasets T5 4-layer Transformer Internal Knowledge D&S RankVicuna (Pradeep et al., 2023a) IR IR Datasets ChatGPT LLaMA Labeling SFT RankZephyr (Pradeep et al., 2023b) IR IR Datasets ChatGPT + GPT4 Mistral Labeling SFT NDR (Mysore et al., 2023) Recommendation Recommendation Datasets GPT3 MPnet-110M Labeling SFT InstructRec (Zhang et al., 2023b) Recommendation 39 instruction templates ChatGPT Flan-T5 Expansion + Self-Knowledge SFT ONCE (Liu et al., 2023c) Recommendation Recommendation Dataset
al., 2024) Evaluation 50 Seed Rubrics GPT4 LLaMA Labeling SFT InstructScore (Xu et al., 2023d) Evaluation Mixed Dataset GPT4 LLaMA Labeling SFT WizardMath (Luo et al., 2023b) Math GSM8k + MATH ChatGPT LLaMA Expansion + Feedback SFT + RL Mammoth (Yue et al., 2023a) Math/TP Mixed Math Dataset GPT4 LLaMA Labeling SFT Mixed Distill (Chenglin et al., 2023) Math/TP SVAMP + GSM8K + ASDIV + StrategyQA ChatGPT LLaMa Labeling SFT WizardCoder (Luo et al., 2023a) Code Code Alpaca Data ChatGPT StarCoder Expansion SFT Magicoder (Wei et al., 2023) Code Existing Source Codes ChatGPT LLaMA Curation SFT WaveCoder (Yu et al., 2024) Code Existing Source Codes GPT4 LLaMA Curation SFT Code Alpaca (Chaudhary, 2023) Code Code Instructions ChatGPT LLaMA Expansion + Self-Knowledge SFT Code Llama (Rozi `ere et al., 2023) Code Human-written Instructions LLaMA LLaMA Expansion + Self-Knowledge SFT Code Clean (Jain et al., 2023)
--------

### Introduction

Vision-Language Model Preprocessing

### Vision-Language COCO

Vision-Language COCO dataset used for Vision-Language model training and evaluation. Contains 22 classes of objects and 1000+ images.

### Vision-Language Visual Genome

Visual Genome dataset used for Vision-Language model training. Contains 1000+ images and 1000+ entities.

### Vision-Language + COCO

Visual-Language dataset used for Vision-Language model training. Combines Visual Genome and COCO datasets.

### Vision-Language LVIS-Instruct4V

Visual-Language-Instruct4V dataset used for Vision-Language model training. Contains 1000+ images and 1000+ entities.

### Vision-Language LVIS

Visual-Language dataset used for Vision-Language model training. Contains 1000+ images and 1000+ entities.

### Vision-Language LAION

Vision-Language dataset used for Vision-Language model training. Contains 1000+ images and 1000+ entities.

### Vision-Language Macaw-LLM

Vision-Language dataset used for Vision-Language model training. Contains 1000+ images and 1000+ entities. Multimodal data with image/video captions.

### Vision-Language MIMIC-IT

Vision-Language dataset used for Vision-Language model training. Contains 1000+ images and 1000+ entities. Multimodal data with image/video captions.

### Vision-Language ChatBridge

Vision-Language dataset used for Vision-Language model training. Task-specific multimodal data with image/video captions.

### Vision-Language

Vision-Language model used for various applications. Model preprocessed for Vision-Language tasks.

### Task-Specific/Multimodal-Chat

Task-specific multimodal data with image/video captions used for various applications. Model preprocessed for Task-Specific/Multimodal-Chat tasks.

### Table 1: A summary of skill distillation works

| Method | Description |
| --- | --- |
| Instruction Following | Model follows instructions from teacher in a sequence of images |
| MD: Multi-turn Dialogue | Model engages in multi-turn conversation with user |
| TP: Think Pattern | Model uses pattern recognition to understand user intent |
| RAG: Retrieval-Augmented Generation | Model retrieves relevant information from memory and generates text |
| NLU: Natural Language Understanding | Model understands user input and generates text |
| NLG: Natural Language Generation | Model generates text based on user input |
, such as prefacing machine translation data with "Translate this sentence to Spanish:"

Manual data creation is labor-intensive, while template-based transformation lacks diversity in instructions and may not align well with natural human input.

LLMs like GPT-4 offer an efficient alternative for creating diverse and controlled SFT data by their capabilities of in-context learning and instruction following.

Most relevant works use OpenAI's GPT series models to generate prompt-response data pairs and then train the student LLMs by supervised fine-tuning.

Basic Instructions. Self-Instruct
Leverages in-context learning capability
sk-agnostic instructions for various domains and applications
2. Create a machine learning model that can learn from the database and generate instructions for new tasks
3. Introduce a filtering stage to eliminate redundant or similar instructions
4. Utilize a robust text-davinci model to generate high-quality instructions
5. Train the model on a large dataset of expert-written instructions for novel tasks
6. Implement a self-instruction method to enable the model to follow instructions
7. Introduce a topic-guided instruction generation method to enhance diversity in instruction data
8. Develop a method for gathering 3.5K common topics from Wikipedia to serve as guidance
9. Create a system for generating instructions based on the gathered topics
10. Evaluate the performance of the system using a comprehensive evaluation metric
n exhibit low to moderate complexity. WizardLM introduces Evol-Instruct, a method that gradually transforms instructions into more complex forms through a multi-step evolution process, increasing difficulty levels and expanding diversity of topics. Four rounds of evolution using the OpenAI ChatGPT API resulted in a dataset of 250k complex instructions. WizardLM, the LLaMA 7B model, was trained on this dataset, achieving a win rate 7.9% higher than ChatGPT in the high-difficulty section
Fusion (Guo et al., 2023c) further uses teacher LLMs to increase the complexity by fusing two distinct evolved instructions. This concept of “evolving” instructions has been extended to distill specific skills such as coding (Luo et al., 2023a) and mathematics (Luo et al., 2023b). Human Instructions. In contrast to works that rely on generating instructions from ChatGPT, which may lack diversity and have gaps with real human instructions, Vicuna (Chiang et al., 2023) and Koala (Geng et al., 2023) showcase impressive performance by using human conversations and natural instructions from community-contributed conversations.
troduce a system message (e.g., "explain like I'm five, think step-by-step") to guide students. This message will prompt GPT-4 to provide explanation traces that elucidate the teacher's reasoning process. This approach will train student models to identify the most effective solution strategy for each task, guided by Orca's performance. This will significantly improve the models' ability to follow instructions that involve reasoning.
meta-information. The UltraLLaMA model, fine-tuned on this data, consistently surpasses other open-source models. The Phi series models prioritize data quality and employ synthetic methods to generate data of "textbook quality" to enhance the learning experience for smaller models. Notably, Phi exhibits the ability to follow instructions effectively even without specific instruction fine-tuning. Phi-2, with 2.7 billion parameters, outperforms Mistral and Llama-2 models with 7B and 13B parameters across various benchmark evaluations

cts of instruction-following ability, including diversity and complexity. However, student models trained on instruction data expanded by ChatGPT often mimic ChatGPT's style without replicating its factual accuracy.

Achieving a more capable instruction-following capability requires a stronger teacher model and access to diverse, high-quality instruction data, such as the Orca (Mukherjee et al., 2023; Mitra et al., 2023) dataset, which incorporates extensive task instructions from the Flan 2022 Collection (Longpre et al., 2023).
ity for engaging in multi-turn dialogues. Vicuna, for instance, is a chat model exclusively trained on ShareGPT data, achieving a high MT-Bench score despite its sole training source being ShareGPT.
023) enhance the quality of multi-turn data from ShareGPT by generating self-feedback on model responses and iteratively refining the responses based on the received feedback. To enhance the multi-turn capabilities of student models, another line of research focuses on expanding conversational datasets through self-chat and using them to train smaller models (Xu et al., 2023b; Ding et al., 2023b; Tunstall et al., 2023). For instance, Xu et al. (2023b) initiate their work by using questions sourced from Quora and Stack Overflow as seeds, resulting in the collection of 111.5k dialogues through self-chat. They employ parameter-efficient tuning to train a chat model named Baize. Ding et al. (2023b) construct a significantly larger dataset called UltraChat, comprising 1.5 million high-quality multi-turn dialogues.
model, resulting in the creation of UltraLLaMA, a powerful chat model that consistently outperforms other open-source chat models, including Vicuna and Baize.
els, particularly in the retrieval stage. By incorporating both instruction and grounding information into the search results, the method aims to enhance the model's ability to prioritize relevant information. This process involves inputting the retrieved passage along with the ground truth response into the entailment model to label each result for relevance.
ing the quality of retrieved results using a critic model.
ed for retrieval using few-shot demonstrations; predict reflection token ras follows p(r|I, x, y)
Fee has developed a system that continuously refines its answers until it produces high-quality responses in a single inference.
particular, enables student models to adapt to different problem-solving strategies by learning from discrepancies between smaller and larger models. This approach fosters a deeper understanding of reasoning capabilities, as students learn to revise and reflect on their approach.

Moreover, Zhang et al. (2023a) has proposed a novel method to learn domain-specific knowledge and preferences for question-answering tasks, leveraging both LLMs and human evaluation. Notably, the DEBATunE model has been designed to improve the controllability of LLMs in generating statements on contentious topics by engaging two agents in structured debates.

Both approaches have demonstrated significant potential in enhancing the capabilities of large language models (LLMs) in various domains.
curate outcomes, aligning with human preferences, enabling them to assist in various tasks without demanding higher-level demands.

**Early Methods**

Early methods primarily rely on human input, utilizing human feedback to refine their performance.
